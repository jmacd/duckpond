//! Tests for large Parquet file storage integration
//!
//! These tests verify that large Parquet files are handled correctly
//! with the large file storage system.

use crate::arrow::{ForArrow, ParquetExt};
use crate::{EntryType, memory::new_fs};
use arrow::datatypes::{DataType, Field, FieldRef};
use serde::{Deserialize, Serialize};
use std::sync::Arc;

/// The 64 KiB threshold for large file storage (copied from tlogfs)
const LARGE_FILE_THRESHOLD: usize = 64 * 1024;

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
struct LargeTestRecord {
    id: i64,
    name: String,
    description: String, // Add longer text field
    data: Vec<u8>,       // Add binary data field
    score: Option<f64>,
}

impl ForArrow for LargeTestRecord {
    fn for_arrow() -> Vec<FieldRef> {
        vec![
            Arc::new(Field::new("id", DataType::Int64, false)),
            Arc::new(Field::new("name", DataType::Utf8, false)),
            Arc::new(Field::new("description", DataType::Utf8, false)),
            Arc::new(Field::new("data", DataType::Binary, false)),
            Arc::new(Field::new("score", DataType::Float64, true)),
        ]
    }
}

#[tokio::test]
async fn test_parquet_file_size_estimation() -> Result<(), Box<dyn std::error::Error>> {
    let fs = new_fs().await;
    let wd = fs.root().await?;

    // Create different sized datasets to see what triggers large file storage
    let sizes = vec![100, 500, 1000, 2500, 5000, 10000];
    
    for size in sizes {
        let large_data: Vec<LargeTestRecord> = (0..size)
            .map(|i| LargeTestRecord {
                id: i as i64,
                name: format!("User_{}_with_longer_name_to_increase_size", i),
                description: format!("This is a longer description for user {} to increase the size of each record and make the parquet file larger. We need to trigger large file storage which happens at {} bytes.", i, LARGE_FILE_THRESHOLD),
                data: vec![42u8; 100], // 100 bytes of binary data per record
                score: if i % 3 == 0 { None } else { Some((i as f64) * 0.1) },
            })
            .collect();

        let test_path = format!("test_size_{}.parquet", size);
        
        // Write the dataset
        wd.create_table_from_items(&test_path, &large_data, EntryType::FileTable).await?;
        
        // Read the raw file content to check size
        let content = wd.read_file_path_to_vec(&test_path).await?;
        let file_size = content.len();
        
        println!("Dataset size: {} records", size);
        println!("Parquet file size: {} bytes ({:.2} KiB)", file_size, file_size as f64 / 1024.0);
        println!("Large file threshold: {} bytes ({:.2} KiB)", LARGE_FILE_THRESHOLD, LARGE_FILE_THRESHOLD as f64 / 1024.0);
        println!("Triggers large file storage: {}", file_size >= LARGE_FILE_THRESHOLD);
        println!();
        
        // Test that we can read it back correctly
        let read_data: Vec<LargeTestRecord> = wd.read_table_as_items(&test_path).await?;
        assert_eq!(large_data.len(), read_data.len());
        
        if file_size >= LARGE_FILE_THRESHOLD {
            println!("ðŸŽ¯ Found a dataset size that triggers large file storage!");
            break;
        }
    }

    Ok(())
}

#[tokio::test]
async fn test_guaranteed_large_parquet_file() -> Result<(), Box<dyn std::error::Error>> {
    let fs = new_fs().await;
    let wd = fs.root().await?;

    // Create a dataset that definitely exceeds 64 KiB
    let record_count = 20000; // Much larger dataset
    let large_data: Vec<LargeTestRecord> = (0..record_count)
        .map(|i| LargeTestRecord {
            id: i as i64,
            name: format!("User_{}_with_very_long_name_to_ensure_large_file_storage_is_triggered_definitely", i),
            description: format!("This is an extremely long description for user {} designed to create a large parquet file that will definitely exceed the 64 KiB threshold for large file storage. We're repeating this text multiple times to ensure size: {}", i, "padding ".repeat(20)),
            data: vec![42u8; 500], // 500 bytes of binary data per record
            score: if i % 3 == 0 { None } else { Some((i as f64) * 0.1) },
        })
        .collect();

    let test_path = "guaranteed_large.parquet";
    
    // Write the dataset
    wd.create_table_from_items(test_path, &large_data, EntryType::FileTable).await?;
    
    // Read the raw file content to check size
    let content = wd.read_file_path_to_vec(test_path).await?;
    let file_size = content.len();
    
    println!("ðŸŽ¯ GUARANTEED LARGE FILE TEST");
    println!("Dataset size: {} records", record_count);
    println!("Parquet file size: {} bytes ({:.2} KiB)", file_size, file_size as f64 / 1024.0);
    println!("Large file threshold: {} bytes ({:.2} KiB)", LARGE_FILE_THRESHOLD, LARGE_FILE_THRESHOLD as f64 / 1024.0);
    println!("Triggers large file storage: {}", file_size >= LARGE_FILE_THRESHOLD);
    
    // This should definitely trigger large file storage
    assert!(file_size >= LARGE_FILE_THRESHOLD, 
        "Expected file size {} to exceed threshold {}", file_size, LARGE_FILE_THRESHOLD);
    
    // Verify we can still read it back correctly
    let read_data: Vec<LargeTestRecord> = wd.read_table_as_items(test_path).await?;
    assert_eq!(large_data.len(), read_data.len());
    
    // Spot check some records
    assert_eq!(large_data[0], read_data[0]);
    assert_eq!(large_data[10000], read_data[10000]);
    assert_eq!(large_data[19999], read_data[19999]);
    
    println!("âœ… Large Parquet file test successful!");
    println!("   Large file storage working correctly with {} KiB file", file_size / 1024);
    
    Ok(())
}
