#!/bin/bash
# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Caspar Water Company
#
# duckpond-emergency - Emergency file extraction from DuckPond backups
#
# This script extracts files from DuckPond Delta Lake backups using only
# DuckDB - no pond binary required. Use this for disaster recovery when
# the pond software is unavailable.
#
# REQUIREMENTS:
#   - DuckDB CLI (https://duckdb.org/)
#   - b3sum for BLAKE3 verification (optional)
#
# USAGE:
#   duckpond-emergency <backup-path> list
#   duckpond-emergency <backup-path> extract <file-pattern> [output-dir]
#   duckpond-emergency <backup-path> verify <file-pattern>
#   duckpond-emergency <backup-path> export-all <output-dir>
#
# EXAMPLES:
#   # List all files in backup
#   duckpond-emergency /path/to/backup list
#
#   # Extract a specific file
#   duckpond-emergency /path/to/backup extract "_delta_log/00000000000000000001.json"
#
#   # Extract all delta logs to a directory
#   duckpond-emergency /path/to/backup extract "_delta_log%" ./extracted/
#
#   # Verify all files have valid checksums
#   duckpond-emergency /path/to/backup verify
#
# S3/MinIO USAGE:
#   Export these environment variables first:
#     export AWS_ENDPOINT_URL="http://localhost:9000"
#     export AWS_REGION="us-east-1"
#     export AWS_ACCESS_KEY_ID="minioadmin"
#     export AWS_SECRET_ACCESS_KEY="minioadmin"
#
#   Then use s3:// URLs:
#   duckpond-emergency s3://bucket/backup list
#
# IMPLEMENTATION NOTES:
#   This script reads DuckPond backups directly from parquet files WITHOUT
#   using the Delta Lake protocol. This is intentional for disaster recovery:
#
#   1. STORAGE FORMAT:
#      DuckPond's remote backup stores data in a Delta Lake table with this schema:
#        - bundle_id: Partition column (e.g., "FILE-META-2026-02-04-1")
#        - path: Original file path in the pond
#        - pond_txn_id: Transaction sequence number
#        - chunk_id: Chunk sequence (0, 1, 2...) for multi-chunk files
#        - chunk_data: Binary blob containing the actual file data
#        - root_hash: BLAKE3 hash of the complete file
#        - total_size: File size in bytes
#
#   2. DIRECTORY STRUCTURE:
#      backup_path/
#        bundle_id=FILE-META-2026-02-04-1/
#          *.parquet  <- Contains rows for all files in txn 1
#        bundle_id=FILE-META-2026-02-04-2/
#          *.parquet  <- Contains rows for all files in txn 2
#        _delta_log/
#          00000000000000000001.json  <- Delta commit logs (we ignore these)
#
#   3. WHY read_parquet() INSTEAD OF delta_scan():
#      - delta_scan() requires the Delta Lake extension which has DeltaKernel FFI
#      - DeltaKernel has bugs with predicate pushdown on certain queries
#      - read_parquet() with hive_partitioning=true reads the files directly
#      - This is more robust for emergency recovery scenarios
#
#   4. BLOB EXTRACTION:
#      DuckDB can't directly export BLOB columns to binary files. We work around
#      this by:
#        a) Using to_base64(chunk_data) to convert blobs to base64 strings
#        b) Piping each base64 line through `base64 -d` to decode
#        c) Appending decoded chunks in chunk_id order to reconstruct the file
#
#   5. STDIN HANDLING:
#      Nested while loops in bash share stdin. To prevent inner duckdb commands
#      from consuming the outer loop's input, we:
#        a) Write file lists to temp files
#        b) Read from file descriptor 3 for the outer loop
#        c) Capture duckdb output to variables before processing
#
# Note: set -e is disabled to allow proper loop error handling
# set -e

VERSION="1.0.0"
SCRIPT_NAME="duckpond-emergency"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

#############################
# HELPER FUNCTIONS
#############################

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[OK]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" >&2
}

usage() {
    cat << EOF
$SCRIPT_NAME v$VERSION - Emergency file extraction from DuckPond backups

USAGE:
    $SCRIPT_NAME <backup-path> <command> [args...]

COMMANDS:
    list                    List all files in the backup
    extract <pattern> [dir] Extract files matching pattern to directory
    verify [pattern]        Verify BLAKE3 checksums of files
    export-all <dir>        Export all files to directory
    info                    Show backup metadata

OPTIONS:
    -h, --help             Show this help message
    -v, --version          Show version

BACKUP PATH:
    Local:  /path/to/backup or file:///path/to/backup
    S3:     s3://bucket/path (set AWS_* env vars first)

EXAMPLES:
    # List files
    $SCRIPT_NAME /backup list

    # Extract specific file
    $SCRIPT_NAME /backup extract "part_id=abc/file.parquet" ./out/

    # Extract all files matching pattern (SQL LIKE syntax: % = wildcard)
    $SCRIPT_NAME /backup extract "_delta_log%" ./delta_logs/

    # Verify all checksums
    $SCRIPT_NAME /backup verify

    # Export everything
    $SCRIPT_NAME /backup export-all ./full_restore/

S3/MINIO SETUP:
    export AWS_ENDPOINT_URL="http://localhost:9000"  # For MinIO
    export AWS_REGION="us-east-1"
    export AWS_ACCESS_KEY_ID="your-key"
    export AWS_SECRET_ACCESS_KEY="your-secret"
    $SCRIPT_NAME s3://bucket/backup list

EOF
    exit 0
}

check_duckdb() {
    if ! command -v duckdb &>/dev/null; then
        log_error "DuckDB is not installed."
        echo ""
        echo "Install DuckDB:"
        echo "  macOS:  brew install duckdb"
        echo "  Linux:  curl -LO https://github.com/duckdb/duckdb/releases/latest/download/duckdb_cli-linux-amd64.zip"
        echo "          unzip duckdb_cli-linux-amd64.zip && sudo mv duckdb /usr/local/bin/"
        echo ""
        exit 1
    fi
}

# Build DuckDB preamble for S3 or local access
build_duckdb_preamble() {
    local backup_path="$1"
    
    # No delta extension needed - we read raw parquet files directly
    
    if [[ "$backup_path" == s3://* ]]; then
        echo "INSTALL httpfs; LOAD httpfs;"
        
        if [ -n "$AWS_REGION" ]; then
            echo "SET s3_region='$AWS_REGION';"
        else
            echo "SET s3_region='us-east-1';"
        fi
        
        if [ -n "$AWS_ENDPOINT_URL" ]; then
            local endpoint="${AWS_ENDPOINT_URL#http://}"
            endpoint="${endpoint#https://}"
            echo "SET s3_endpoint='$endpoint';"
            echo "SET s3_url_style='path';"
            if [[ "$AWS_ENDPOINT_URL" == http://* ]]; then
                echo "SET s3_use_ssl=false;"
            fi
        fi
        
        if [ -n "$AWS_ACCESS_KEY_ID" ]; then
            echo "SET s3_access_key_id='$AWS_ACCESS_KEY_ID';"
        fi
        
        if [ -n "$AWS_SECRET_ACCESS_KEY" ]; then
            echo "SET s3_secret_access_key='$AWS_SECRET_ACCESS_KEY';"
        fi
    fi
}

# Build the glob pattern for reading all parquet files in a backup
# Structure: backup_path/bundle_id=*/**.parquet
parquet_glob() {
    local backup_path="$1"
    # Remove trailing slash if present
    backup_path="${backup_path%/}"
    echo "${backup_path}/bundle_id=*/*.parquet"
}

#############################
# COMMANDS
#############################

cmd_list() {
    local backup_path="$1"
    
    log_info "Listing files in backup: $backup_path"
    echo ""
    
    local preamble
    local parquet_path
    preamble=$(build_duckdb_preamble "$backup_path")
    parquet_path=$(parquet_glob "$backup_path")
    
    duckdb -c "
$preamble
SELECT 
    path,
    total_size as size_bytes,
    pond_txn_id as txn,
    root_hash
FROM read_parquet('$parquet_path', hive_partitioning=true)
GROUP BY path, total_size, pond_txn_id, root_hash
ORDER BY path;
"
}

cmd_info() {
    local backup_path="$1"
    
    log_info "Backup information: $backup_path"
    echo ""
    
    local preamble
    local parquet_path
    preamble=$(build_duckdb_preamble "$backup_path")
    parquet_path=$(parquet_glob "$backup_path")
    
    echo "=== Summary ==="
    duckdb -c "
$preamble
SELECT 
    COUNT(DISTINCT path) as total_files,
    COUNT(*) as total_chunks,
    AVG(total_size) as avg_file_size,
    MIN(pond_txn_id) as min_txn,
    MAX(pond_txn_id) as max_txn
FROM read_parquet('$parquet_path', hive_partitioning=true);
"
    
    echo ""
    echo "=== Files by Transaction ==="
    duckdb -c "
$preamble
SELECT 
    pond_txn_id as txn,
    COUNT(DISTINCT path) as files,
    SUM(total_size) as total_bytes
FROM read_parquet('$parquet_path', hive_partitioning=true)
GROUP BY pond_txn_id
ORDER BY pond_txn_id;
"
}

cmd_extract() {
    local backup_path="$1"
    local pattern="$2"
    local output_dir="${3:-.}"
    
    mkdir -p "$output_dir"
    
    log_info "Extracting files matching: $pattern"
    log_info "Output directory: $output_dir"
    echo ""
    
    local preamble
    local parquet_path
    preamble=$(build_duckdb_preamble "$backup_path")
    parquet_path=$(parquet_glob "$backup_path")
    
    # Get list of files matching pattern - write to temp file to avoid stdin issues
    local files_tmp
    files_tmp=$(mktemp)
    trap "rm -f '$files_tmp'" RETURN
    
    duckdb -noheader -csv -c "
$preamble
SELECT DISTINCT bundle_id, path, pond_txn_id, root_hash
FROM read_parquet('$parquet_path', hive_partitioning=true)
WHERE path LIKE '$pattern'
ORDER BY path;
" </dev/null 2>/dev/null > "$files_tmp"
    
    if [ ! -s "$files_tmp" ]; then
        log_warn "No files match pattern: $pattern"
        return 1
    fi
    
    local file_count
    file_count=$(wc -l < "$files_tmp" | tr -d ' ')
    log_info "Found $file_count files to extract"
    
    local count=0
    local failed=0
    
    # Open file descriptor 3 for reading the file list
    exec 3< "$files_tmp"
    
    # Read from file descriptor 3 to avoid stdin interference from inner commands
    while IFS=',' read -r bundle_id path pond_txn_id root_hash <&3; do
        # Skip empty lines
        [ -z "$bundle_id" ] && continue
        
        # Create safe filename
        local safe_name
        safe_name=$(echo "$path" | tr '/' '_' | tr '=' '_')
        local output_file="$output_dir/$safe_name"
        
        log_info "Extracting: $path"
        
        # Extract using DuckDB - get each chunk as base64, decode and concatenate
        # This handles multi-chunk files correctly by processing each chunk in order
        rm -f "$output_file" 2>/dev/null
        
        local chunk_count=0
        local chunk_failed=0
        
        # Get chunks as base64, one per line, in chunk_id order
        # Using read_parquet directly instead of delta_scan to avoid DeltaKernel issues
        # Note: duckdb errors are handled - don't let set -e kill the loop
        local duckdb_output
        duckdb_output=$(duckdb -noheader -list -c "
$preamble
SELECT to_base64(chunk_data)
FROM read_parquet('$parquet_path', hive_partitioning=true)
WHERE path = '$path'
  AND pond_txn_id = $pond_txn_id
ORDER BY chunk_id;
" </dev/null 2>&1) || true
        
        while IFS= read -r b64_chunk; do
            [ -z "$b64_chunk" ] && continue
            # Decode and append to output file
            if echo "$b64_chunk" | base64 -d >> "$output_file" 2>/dev/null; then
                ((chunk_count++))
            else
                ((chunk_failed++))
            fi
        done <<< "$duckdb_output"
        
        if [ "$chunk_count" -gt 0 ] && [ "$chunk_failed" -eq 0 ] && [ -f "$output_file" ] && [ -s "$output_file" ]; then
            local size
            size=$(wc -c < "$output_file" 2>/dev/null | tr -d ' ')
            log_success "  -> $output_file ($size bytes, $chunk_count chunks)"
            
            # Verify BLAKE3 if available
            if command -v b3sum &>/dev/null && [ -n "$root_hash" ]; then
                local actual_hash
                actual_hash=$(b3sum "$output_file" 2>/dev/null | cut -d' ' -f1)
                if [ "$actual_hash" = "$root_hash" ]; then
                    log_success "  BLAKE3 verified: $root_hash"
                else
                    log_warn "  BLAKE3 mismatch! Expected: $root_hash, Got: $actual_hash"
                    ((failed++))
                fi
            fi
            ((count++))
        else
            log_error "  Failed to extract: $path (chunks: $chunk_count, failed: $chunk_failed)"
            ((failed++))
        fi
        
    done
    
    # Close file descriptor
    exec 3<&-
    
    echo ""
    log_info "Extracted $count files to $output_dir"
    if [ $failed -gt 0 ]; then
        log_warn "$failed files had errors or verification failures"
        return 1
    fi
}

cmd_verify() {
    local backup_path="$1"
    local pattern="${2:-%}"
    
    log_info "Verifying files matching: $pattern"
    echo ""
    
    if ! command -v b3sum &>/dev/null; then
        log_error "b3sum is required for verification but not installed."
        echo "Install with: cargo install b3sum"
        return 1
    fi
    
    local preamble
    local parquet_path
    preamble=$(build_duckdb_preamble "$backup_path")
    parquet_path=$(parquet_glob "$backup_path")
    
    # Get list of files - write to temp file to avoid stdin issues
    local files_tmp
    files_tmp=$(mktemp)
    
    duckdb -noheader -csv -c "
$preamble
SELECT DISTINCT bundle_id, path, pond_txn_id, root_hash
FROM read_parquet('$parquet_path', hive_partitioning=true)
WHERE path LIKE '$pattern'
ORDER BY path;
" </dev/null 2>/dev/null > "$files_tmp"
    
    local count=0
    local verified=0
    local failed=0
    
    local tmpfile
    tmpfile=$(mktemp)
    
    # Open file descriptor 3 for reading the file list
    exec 3< "$files_tmp"
    
    while IFS=',' read -r bundle_id path pond_txn_id root_hash <&3; do
        [ -z "$bundle_id" ] && continue
        ((count++))
        
        echo -n "Verifying: $path ... "
        
        # Extract to temp file using base64 approach
        rm -f "$tmpfile" 2>/dev/null
        local chunk_count=0
        
        local duckdb_output
        duckdb_output=$(duckdb -noheader -list -c "
$preamble
SELECT to_base64(chunk_data)
FROM read_parquet('$parquet_path', hive_partitioning=true)
WHERE path = '$path'
  AND pond_txn_id = $pond_txn_id
ORDER BY chunk_id;
" </dev/null 2>&1) || true
        
        while IFS= read -r b64_chunk; do
            [ -z "$b64_chunk" ] && continue
            echo "$b64_chunk" | base64 -d >> "$tmpfile" 2>/dev/null
            ((chunk_count++)) || true
        done <<< "$duckdb_output"
        
        if [ "$chunk_count" -gt 0 ] && [ -f "$tmpfile" ] && [ -s "$tmpfile" ]; then
            local actual_hash
            actual_hash=$(b3sum "$tmpfile" 2>/dev/null | cut -d' ' -f1)
            
            if [ "$actual_hash" = "$root_hash" ]; then
                echo -e "${GREEN}OK${NC}"
                ((verified++)) || true
            else
                echo -e "${RED}MISMATCH${NC}"
                echo "  Expected: $root_hash"
                echo "  Actual:   $actual_hash"
                ((failed++)) || true
            fi
        else
            echo -e "${RED}EXTRACT FAILED${NC}"
            ((failed++)) || true
        fi
        
    done
    
    # Close file descriptor and cleanup
    exec 3<&-
    rm -f "$files_tmp" "$tmpfile" 2>/dev/null
    
    echo ""
    echo "=== Verification Summary ==="
    echo "Total files:    $count"
    echo "Verified OK:    $verified"
    echo "Failed:         $failed"
    
    if [ $failed -gt 0 ]; then
        return 1
    fi
}

cmd_export_all() {
    local backup_path="$1"
    local output_dir="$2"
    
    if [ -z "$output_dir" ]; then
        log_error "Output directory required"
        echo "Usage: $SCRIPT_NAME $backup_path export-all <output-dir>"
        return 1
    fi
    
    cmd_extract "$backup_path" "%" "$output_dir"
}

#############################
# MAIN
#############################

main() {
    # Parse global options
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -h|--help)
                usage
                ;;
            -v|--version)
                echo "$SCRIPT_NAME v$VERSION"
                exit 0
                ;;
            -*)
                log_error "Unknown option: $1"
                echo "Use --help for usage information"
                exit 1
                ;;
            *)
                break
                ;;
        esac
    done
    
    # Check arguments
    if [ $# -lt 2 ]; then
        log_error "Missing arguments"
        echo "Usage: $SCRIPT_NAME <backup-path> <command> [args...]"
        echo "Use --help for more information"
        exit 1
    fi
    
    local backup_path="$1"
    local command="$2"
    shift 2
    
    # Normalize backup path
    if [[ "$backup_path" != s3://* ]] && [[ "$backup_path" != file://* ]]; then
        # Local path - ensure it exists
        if [ ! -d "$backup_path" ]; then
            log_error "Backup directory not found: $backup_path"
            exit 1
        fi
    fi
    
    check_duckdb
    
    case "$command" in
        list)
            cmd_list "$backup_path"
            ;;
        info)
            cmd_info "$backup_path"
            ;;
        extract)
            cmd_extract "$backup_path" "$@"
            ;;
        verify)
            cmd_verify "$backup_path" "$@"
            ;;
        export-all)
            cmd_export_all "$backup_path" "$@"
            ;;
        *)
            log_error "Unknown command: $command"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
}

main "$@"
